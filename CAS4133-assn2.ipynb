{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\renewcommand{\\thesection}{}\n",
    "\\renewcommand{\\thesubsection}{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAS4133 - Assignment 2 (Due **5/23** at **11:59 PM**)\n",
    "\n",
    "## [Summary]\n",
    "\n",
    "Your goal is to use alignment algorithms such as but not limited to SFT, RLHF, or DPO to achieve a good score vs. your base model on an alignment benchmark. \n",
    "\n",
    "This notebook provides an example SFT + DPO training pipeline, but you have unlimited freedom in utilizing other approaches.\n",
    "\n",
    "## [What to submit]\n",
    "\n",
    "- An `.ipynb` file named: `CAS4133-assn2-studentnumber.ipynb`.  \n",
    "- **[Important]** HuggingFace Repository containing your final model in the format: `${User_Name}/${Repo_Name}`  \n",
    "- Describe and justify **every design choice** in the pipeline (alignment algorithms, hyperparameters, datasets, and models selected).  \n",
    "- A comparison of your model's **benchmark scores** and a **na√Øvely trained model**, using the evaluation pipeline provided here.\n",
    "\n",
    "## [Grading: 10 pts total]\n",
    "\n",
    "Evaluation will be based on **Downstream performance** on private benchmark\n",
    "\n",
    "**Scoring breakdown:**\n",
    "- **+6 pts** Achieved win rate higher than 30% over the base model on undisclosed benchmark evaluation\n",
    "- **+2 pts** Submitted a PDF report with required details in [What to submit] section\n",
    "- **+1 pt** If your model is in top 70% in benchmark evaluation\n",
    "- **+1 pt** If your model is in top 40% in benchmark evaluation\n",
    "- **+1 pt** (*Extra credit*) If your model is in top-3 in benchmark evaluation  \n",
    "  *(Can be transferred to another assignment as bonus)*\n",
    "\n",
    "\n",
    "## [Notes]\n",
    "\n",
    "- Assignment should be done with 1 RTX-3090 GPU (; 24G VRAM)\n",
    "- TA: Hojin Kim / E-mail: hojinkimirl@gmail.com\n",
    "- On TA's setup, the full pipline takes ~3 hours\n",
    "\n",
    "- **HuggingFace Hub submission requirements:**\n",
    "  - If you're using LORA adapters, make sure to merge your model before pushing to huggingface.\n",
    "  - Generate a write-access token beforehand ([official documentation](https://huggingface.co/docs/hub/security-tokens))\n",
    "  - Securely store your token\n",
    "  - Authenticate via terminal:\n",
    "    ```bash\n",
    "    huggingface-cli login\n",
    "        ```\n",
    "\n",
    "**Remark: The Maximum allowed runtime for Assignment 2 is 72 hours per account. Please note that if the total exceeds 72 hours, your assigned credits will be depleted and you will no longer be able to proceed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SFT and DPO Training Pipeline with Unsloth**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Fine-tuning a Llama-3.2-1B model using Supervised Fine-Tuning (SFT) with Unsloth\n",
    "2. Evaluating the SFT model against the base model\n",
    "3. Performing DPO training on the SFT model\n",
    "4. Evaluating the DPO model against the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Create a directory for logs and install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory\n",
    "!mkdir -p ./outputs/logs\n",
    "\n",
    "# Start log\n",
    "!echo \"=== STARTING OPTIMIZED SFT AND DPO RUN ===\" > ./outputs/logs/training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer to use 'uv' which is the future of environment management tools\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install ipykernel==6.29.5\n",
    "# !pip install unsloth==2025.4.3\n",
    "# !pip install unsloth-zoo==2025.4.2\n",
    "# !pip install torch==2.7.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install gdown==5.2.0\n",
    "# !pip install huggingface_hub==0.30.2\n",
    "# !pip install wandb==0.19.9  # Optional\n",
    "# !pip install bitsandbytes==0.45.5\n",
    "# !pip install transformers==4.51.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CAS4133/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ [CHECKPOINT] Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from unsloth.chat_templates import get_chat_template\n",
    "    from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "    from trl import SFTTrainer, DPOTrainer\n",
    "    from peft import PeftModel\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        TrainingArguments, \n",
    "        TextStreamer,\n",
    "        AutoModelForCausalLM,\n",
    "    )\n",
    "    print(\"‚úÖ [CHECKPOINT] Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ImportError: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using device: cuda\n",
      "GPU Info:\n",
      "- Device count: 1\n",
      "- Current device: 0\n",
      "- Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Info:\")\n",
    "    print(f\"- Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"- Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"- Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [CHECKPOINT] Seed set\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "def set_seed(seed=1):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1)\n",
    "print(\"‚úÖ [CHECKPOINT] Seed set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Load model utilizing Unsloth's optimized loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model - this may take a moment...\n",
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ [CHECKPOINT] Model loaded in 105.94s\n",
      "Model config: LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"unsloth_version\": \"2025.5.7\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model - this may take a moment...\")\n",
    "model_load_start = time.time()\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    print(f\"‚úÖ [CHECKPOINT] Model loaded in {time.time() - model_load_start:.2f}s\")\n",
    "    print(\"Model config:\", model.config)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PEFT/LoRA\n",
    "Configure model for Parameter-Efficient Fine-Tuning using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.5.7 patched 16 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [CHECKPOINT] PEFT model created in 2.45s\n"
     ]
    }
   ],
   "source": [
    "peft_start = time.time()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "print(f\"‚úÖ [CHECKPOINT] PEFT model created in {time.time() - peft_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "Prepare the Alpaca dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset formatting function\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51760/51760 [00:00<00:00, 158900.85 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30000/30000 [00:00<00:00, 141601.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [CHECKPOINT] Dataset loaded and processed in 8.34s with 30000 examples\n",
      "\n",
      "Sample from dataset:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help pr...\n",
      "Filtered dataset size: 30000 (original had ~52k)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and process the dataset\n",
    "dataset_start = time.time()\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:30000]\")\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    print(f\"‚úÖ [CHECKPOINT] Dataset loaded and processed in {time.time() - dataset_start:.2f}s with {len(dataset)} examples\")\n",
    "    \n",
    "    # Display a sample\n",
    "    print(\"\\nSample from dataset:\")\n",
    "    print(dataset[0][\"text\"][:500] + \"...\")\n",
    "\n",
    "\n",
    "    print(f\"Filtered dataset size: {len(dataset)} (original had ~52k)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load/process dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training\n",
    "Set up the training configuration using SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n",
      "‚úÖ [MAXIMUM PERFORMANCE CONFIG] Trainer configured for optimal quality training\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFT Trainer\n",
    "output_dir = \"./outputs\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=12,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=2.0,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=50,\n",
    "        logging_first_step=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.02,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        gradient_checkpointing=\"unsloth\",\n",
    "        max_grad_norm=0.3,\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_pin_memory=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ [MAXIMUM PERFORMANCE CONFIG] Trainer configured for optimal quality training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 30,000 | Num Epochs = 2 | Total steps = 1,876\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 6,815,744/1,000,000,000 (0.68% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='1876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 945/1876 29:02 < 28:40, 0.54 it/s, Epoch 1.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_start = time.time()\n",
    "\n",
    "try:\n",
    "    trainer_stats = trainer.train()\n",
    "    print(f\"‚úÖ [CHECKPOINT] Training completed in {time.time() - train_start:.2f}s\")\n",
    "    print(f\"Training stats: {trainer_stats}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "Save the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter\n",
    "save_start = time.time()\n",
    "sft_adapter_path = \"./outputs/sft_lora_adapter\"\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(\n",
    "        sft_adapter_path,\n",
    "        save_adapter=True,\n",
    "        save_config=True\n",
    "    )\n",
    "    print(f\"‚úÖ [CHECKPOINT] SFT model adapter saved in {time.time() - save_start:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-Tuned Model\n",
    "We can test our fine-tuned model with a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"Testing fine-tuned model with a sample prompt...\")\n",
    "\n",
    "try:\n",
    "    # Load the adapter onto a fresh model instance\n",
    "    test_model, test_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=load_in_4bit\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    test_model = PeftModel.from_pretrained(test_model, sft_adapter_path)\n",
    "    \n",
    "    # Create a text streamer for nice output\n",
    "    streamer = TextStreamer(test_tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # Sample prompt\n",
    "    test_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain the concept of fine-tuning in machine learning.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"\\nPrompt:\\n\", test_prompt)\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = test_tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = test_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True,\n",
    "        streamer=streamer\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Testing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Evaluation\n",
    "Now we'll set up a comprehensive evaluation comparing our fine-tuned model against the base model using a judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluation\n",
    "eval_start = time.time()\n",
    "\n",
    "print(\"Loading models for evaluation...\")\n",
    "\n",
    "def load_base_model():\n",
    "    print(\"Loading base model\")\n",
    "    try:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "            max_seq_length=1024,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load base model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_sft_model():\n",
    "    print(\"Loading SFT model\")\n",
    "    try:\n",
    "        base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "            max_seq_length=1024,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, sft_adapter_path)\n",
    "        return peft_model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load SFT model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_judge_model():\n",
    "    print(\"Loading judge model\")\n",
    "    try:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load judge model: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    base_model, base_tokenizer = load_base_model()\n",
    "    sft_model, sft_tokenizer = load_sft_model()\n",
    "    judge_model, judge_tokenizer = load_judge_model()\n",
    "    print(\"‚úÖ [CHECKPOINT] Evaluation models loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load evaluation models: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparative Evaluation\n",
    "Let's run a comparative evaluation between the base model and our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define helper functions for evaluation\n",
    "def format_prompt(instruction):\n",
    "    return f\"\"\"Below is an instruction that describes a task. Write a response that completes the request. You will be judged on the following criteria:\n",
    "    1. **Objectivity**: How objective and formal the tone is. \n",
    "    2. **Instruction Following**: How well the response directly addresses the instruction's requirements.\n",
    "    3. **Specificity**: How specific and long the response is.\n",
    "    4. **Clarity**: How readable and coherent the response is, with no extraneous characters.\n",
    "    5. **Accuracy**: Factual correctness and logical consistency.\n",
    "    6. **Helpfulness**: Practicality and usefulness of the response.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=2048,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the response\n",
    "    return full_response[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judgment(instruction, sft_response, base_response):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two responses to an instruction. Score based on the following criteria, ignoring the order of the responses, prioritizing these metrics:\n",
    "\n",
    "1. **Objectivity**: How objective and formal the tone is.\n",
    "2. **Instruction Following**: How well the response directly addresses the instruction's requirements.\n",
    "3. **Specificity**: How specific and long the response is.\n",
    "4. **Clarity**: How readable and coherent the response is, with no extraneous characters.\n",
    "5. **Accuracy**: Factual correctness and logical consistency.\n",
    "6. **Helpfulness**: Practicality and usefulness of the response.\n",
    "\n",
    "\n",
    "**Special Rule**: If the First Response (SFT Model) is empty, irrelevant, or fails to provide any meaningful answer, score the Second Response (Base Model) as significantly better unless it is also empty or irrelevant.\n",
    "\n",
    "Return ONLY a single number:\n",
    "1 = First response (SFT Model) is significantly better\n",
    "2 = Second response (Base Model) is significantly better\n",
    "0 = Both are comparable (similar quality or minor differences)\n",
    "\n",
    "Instruction:\n",
    "{instruction}\n",
    "\n",
    "First Response (SFT Model):\n",
    "{sft_response}\n",
    "\n",
    "Second Response (Base Model):\n",
    "{base_response}\n",
    "\n",
    "Judgment (1/2/0):\"\"\"\n",
    "    \n",
    "    inputs = judge_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=2048,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = judge_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    verdict = judge_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the last valid digit from the response\n",
    "    verdict_digits = [c for c in verdict.strip() if c.isdigit()]\n",
    "    return int(verdict_digits[-1]) if verdict_digits else 0\n",
    "\n",
    "# Load evaluation dataset\n",
    "print(\"Loading evaluation dataset...\")\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "try:\n",
    "    eval_dataset = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\", trust_remote_code=True)[\"eval\"]\n",
    "    eval_dataset = eval_dataset.shuffle(seed=420).select(range(min(len(eval_dataset), MAX_SAMPLES)))\n",
    "    print(f\"Loaded evaluation dataset with {len(eval_dataset)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load evaluation dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(f\"Running evaluation on {len(eval_dataset)} examples...\")\n",
    "eval_start = time.time()\n",
    "results = []\n",
    "cnt = 0\n",
    "\n",
    "for i, example in enumerate(eval_dataset):\n",
    "    try:\n",
    "        \n",
    "        print(f\"\\nExample {i+1}/{len(eval_dataset)}\")\n",
    "        if cnt%10==0: print(f\"Evaluating: {example['instruction'][:100]}...\")\n",
    "        \n",
    "        prompt = format_prompt(example[\"instruction\"])\n",
    "        base_response = generate_response(base_model, base_tokenizer, prompt)\n",
    "        sft_response = generate_response(sft_model, sft_tokenizer, prompt)\n",
    "\n",
    "        if cnt%10==0:\n",
    "            print(\"\\nBase Model Response:\")\n",
    "            print(base_response)\n",
    "            print(\"\\nSFT Model Response:\")\n",
    "            print(sft_response)\n",
    "        \n",
    "        # Check for empty or non-meaningful responses\n",
    "        base_empty = not base_response.strip() or len(base_response.strip()) < 5 or base_response.strip().lower() in [\"n/a\", \"none\", \"no response\"]\n",
    "        sft_empty = not sft_response.strip() or len(sft_response.strip()) < 5 or sft_response.strip().lower() in [\"n/a\", \"none\", \"no response\"]\n",
    "        \n",
    "        if base_empty and not sft_empty:\n",
    "            verdict = 1\n",
    "            print(\"Base response empty or non-meaningful - verdict: 1 (SFT wins)\")\n",
    "        elif sft_empty and not base_empty:\n",
    "            verdict = 2\n",
    "            print(\"SFT response empty or non-meaningful - verdict: 2 (Base wins)\")\n",
    "        elif base_empty and sft_empty:\n",
    "            verdict = 0\n",
    "            print(\"Both responses empty or non-meaningful - verdict: 0 (tie)\")\n",
    "        elif base_response.strip() == sft_response.strip():\n",
    "            verdict = 0\n",
    "            print(\"Responses identical - verdict: 0 (tie)\")\n",
    "        else:\n",
    "            verdict = get_judgment(example[\"instruction\"], sft_response, base_response)  # Fixed argument order\n",
    "            print(f\"\\nVerdict: {verdict} ({['Tie', 'SFT wins', 'Base wins'][verdict]})\")  # Fixed verdict labels\n",
    "        \n",
    "        results.append({\n",
    "            \"instruction\": example[\"instruction\"],\n",
    "            \"sft_response\": sft_response,\n",
    "            \"base_response\": base_response,\n",
    "            \"verdict\": verdict\n",
    "        })\n",
    "        cnt += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "total = len(results)\n",
    "if total > 0:\n",
    "    base_wins = sum(1 for r in results if r[\"verdict\"] == 2)\n",
    "    sft_wins = sum(1 for r in results if r[\"verdict\"] == 1)\n",
    "    ties = sum(1 for r in results if r[\"verdict\"] == 0)\n",
    "    \n",
    "    print(\"\\n=== Evaluation Summary ===\")\n",
    "    print(f\"Total examples evaluated: {total}\")\n",
    "    print(f\"Base Model wins: {base_wins} ({base_wins/total:.1%})\")\n",
    "    print(f\"SFT Model wins: {sft_wins} ({sft_wins/total:.1%})\")\n",
    "    print(f\"Ties: {ties} ({ties/total:.1%})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation completed in {time.time() - eval_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze SFT Results\n",
    "We can analyze and save the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "verdicts = [r[\"verdict\"] for r in results]  # Changed from sft_results to results to match variable name\n",
    "sft_stats = {\n",
    "    \"base_wins\": sum(1 for v in verdicts if v == 2),  # Using sum() to match evaluation code style\n",
    "    \"sft_wins\": sum(1 for v in verdicts if v == 1),\n",
    "    \"ties\": sum(1 for v in verdicts if v == 0),\n",
    "    \"total\": len(verdicts),\n",
    "    \"sft_win_rate\": sum(1 for v in verdicts if v == 1) / len(verdicts) if verdicts else 0,\n",
    "}\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== SFT Evaluation Summary ===\")\n",
    "print(f\"Total examples evaluated: {sft_stats['total']}\")\n",
    "print(f\"Base Model wins: {sft_stats['base_wins']} ({sft_stats['base_wins']/sft_stats['total']:.1%})\")\n",
    "print(f\"SFT Model wins: {sft_stats['sft_wins']} ({sft_stats['sft_wins']/sft_stats['total']:.1%})\")\n",
    "print(f\"Ties: {sft_stats['ties']} ({sft_stats['ties']/sft_stats['total']:.1%})\")\n",
    "\n",
    "# Save results\n",
    "sft_output_file = \"./outputs/alpacaeval_sft_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for DPO Training\n",
    "Prepare for Direct Preference Optimization (DPO) training using the SFT model as our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SFT model as our policy model\n",
    "print(\"Loading SFT model for DPO training...\")\n",
    "\n",
    "try:\n",
    "    dpo_model, dpo_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    dpo_model = PeftModel.from_pretrained(dpo_model, sft_adapter_path)\n",
    "    \n",
    "    ref_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ [CHECKPOINT] DPO models loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load DPO models: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preference dataset for DPO\n",
    "print(\"Loading preference dataset for DPO...\")\n",
    "try:\n",
    "    NUM_EXAMPLES = 3000\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "    \n",
    "    # Use specified number of examples for training\n",
    "    dpo_dataset = dataset[\"train\"].select(range(min(NUM_EXAMPLES, len(dataset[\"train\"]))))\n",
    "    \n",
    "    # Format the dataset for DPO and perform validation\n",
    "    def format_dpo_example(example):\n",
    "        return {\n",
    "            \"prompt\": example[\"question\"],\n",
    "            \"chosen\": example[\"chosen\"],\n",
    "            \"rejected\": example[\"rejected\"],\n",
    "        }\n",
    "    \n",
    "    def filter_valid_examples(example):\n",
    "        if not example[\"chosen\"].strip() or not example[\"rejected\"].strip():\n",
    "            return False\n",
    "        if len(example[\"question\"]) > 3000:  # ~512 tokens\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    # Format dataset\n",
    "    dpo_dataset = dpo_dataset.map(format_dpo_example)\n",
    "    dpo_dataset = dpo_dataset.filter(filter_valid_examples)\n",
    "    \n",
    "    print(f\"Loaded DPO dataset with {len(dpo_dataset)} examples (from target of {NUM_EXAMPLES})\")\n",
    "    \n",
    "    # Display a sample\n",
    "    print(\"\\nSample from DPO dataset:\")\n",
    "    print(\"Prompt:\", dpo_dataset[0][\"prompt\"][:500] + \"...\" if len(dpo_dataset[0][\"prompt\"]) > 500 else dpo_dataset[0][\"prompt\"])\n",
    "    print(\"Chosen response:\", dpo_dataset[0][\"chosen\"][:500] + \"...\" if len(dpo_dataset[0][\"chosen\"]) > 500 else dpo_dataset[0][\"chosen\"])\n",
    "    print(\"Rejected response:\", dpo_dataset[0][\"rejected\"][:500] + \"...\" if len(dpo_dataset[0][\"rejected\"]) > 500 else dpo_dataset[0][\"rejected\"])\n",
    "    \n",
    "    # Dataset statistics\n",
    "    prompt_lengths = [len(example[\"prompt\"]) for example in dpo_dataset]\n",
    "    chosen_lengths = [len(example[\"chosen\"]) for example in dpo_dataset]\n",
    "    rejected_lengths = [len(example[\"rejected\"]) for example in dpo_dataset]\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Average prompt length: {sum(prompt_lengths)/len(prompt_lengths):.1f} chars\")\n",
    "    print(f\"Average chosen response length: {sum(chosen_lengths)/len(chosen_lengths):.1f} chars\")\n",
    "    print(f\"Average rejected response length: {sum(rejected_lengths)/len(rejected_lengths):.1f} chars\")\n",
    "    \n",
    "    # Pre-tokenize the dataset for DPOTrainer\n",
    "    def tokenize_dataset(example):\n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = dpo_tokenizer(example[\"prompt\"], truncation=True, max_length=256, padding=False)\n",
    "        \n",
    "        # Tokenize chosen and rejected responses\n",
    "        chosen_tokens = dpo_tokenizer(example[\"chosen\"], truncation=True, max_length=max_seq_length, padding=False)\n",
    "        rejected_tokens = dpo_tokenizer(example[\"rejected\"], truncation=True, max_length=max_seq_length, padding=False)\n",
    "        \n",
    "        return {\n",
    "            \"prompt\": example[\"prompt\"],\n",
    "            \"chosen\": example[\"chosen\"],\n",
    "            \"rejected\": example[\"rejected\"],\n",
    "            \"input_ids\": prompt_tokens[\"input_ids\"],\n",
    "            \"attention_mask\": prompt_tokens[\"attention_mask\"],\n",
    "            \"chosen_input_ids\": chosen_tokens[\"input_ids\"],\n",
    "            \"chosen_attention_mask\": chosen_tokens[\"attention_mask\"],\n",
    "            \"rejected_input_ids\": rejected_tokens[\"input_ids\"],\n",
    "            \"rejected_attention_mask\": rejected_tokens[\"attention_mask\"],\n",
    "        }\n",
    "        \n",
    "    tokenized_dataset = dpo_dataset.map(\n",
    "        tokenize_dataset,\n",
    "        batched=False,\n",
    "        num_proc=4,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Dataset tokenized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load/process DPO dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DPO Models\n",
    "We need to prepare the SFT model and a reference model for DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SFT model as our policy model\n",
    "print(\"Loading SFT model for DPO training...\")\n",
    "\n",
    "try:\n",
    "    dpo_model, dpo_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    dpo_model = PeftModel.from_pretrained(dpo_model, sft_adapter_path)\n",
    "    \n",
    "    ref_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ [CHECKPOINT] DPO models loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load DPO models: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure DPO Training\n",
    "Set up the DPO trainer with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DPO trainer\n",
    "dpo_output_dir = \"./outputs/dpo_model\"\n",
    "# Define DPO training arguments\n",
    "dpo_training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=2.0,\n",
    "    learning_rate=1e-7,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    seed=3407,\n",
    "    output_dir=dpo_output_dir,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=2.0,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_training_args,\n",
    "    beta=0.2,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=dpo_tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=256,\n",
    "    generate_during_eval=True,\n",
    ")\n",
    "print(\"‚úÖ [CHECKPOINT] DPO trainer configured with optimized hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DPO Training\n",
    "Perform DPO training on the SFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with DPO\n",
    "print(\"Starting DPO training...\")\n",
    "dpo_train_start = time.time()\n",
    "\n",
    "try:\n",
    "    dpo_trainer.train()\n",
    "    print(f\"‚úÖ [CHECKPOINT] DPO training completed in {time.time() - dpo_train_start:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DPO training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DPO Model\n",
    "Save the DPO-trained model adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DPO adapter\n",
    "dpo_adapter_path = \"./outputs/dpo_lora_adapter\"\n",
    "\n",
    "try:\n",
    "    dpo_trainer.model.save_pretrained(\n",
    "        dpo_adapter_path,\n",
    "        save_adapter=True,\n",
    "        save_config=True\n",
    "    )\n",
    "    print(f\"‚úÖ [CHECKPOINT] DPO model adapter saved to {dpo_adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save DPO model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate DPO Model\n",
    "Now let's evaluate the DPO model against the base model using the same evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Clear GPU cache first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 1. Load base model (4-bit quantized)\n",
    "model, dpo_eval_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=1024,  # Reduce if OOM (original: 2048)\n",
    "    dtype=torch.float16,  # Use float16 for efficiency\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# 2. Define a custom device_map to offload some layers to CPU\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,  # GPU\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    # ... (assign more layers to GPU if possible)\n",
    "    \"model.norm\": \"cpu\",  # Offload norm layer to CPU\n",
    "    \"lm_head\": \"cpu\",     # Offload final layer to CPU\n",
    "}\n",
    "\n",
    "# 3. Load DPO adapter with manual offloading\n",
    "try:\n",
    "    dpo_eval_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        dpo_adapter_path,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    print(\"‚úÖ [SUCCESS] DPO model loaded with CPU offloading!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå [ERROR] Failed to load DPO adapter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run DPO evaluation\n",
    "print(f\"Running evaluation on {len(eval_dataset)} examples...\")\n",
    "eval_start = time.time()\n",
    "results = []\n",
    "cnt = 0\n",
    "\n",
    "for i, example in enumerate(eval_dataset):\n",
    "    try:\n",
    "        print(f\"\\nExample {i+1}/{len(eval_dataset)}\")\n",
    "        #if cnt%10==0: \n",
    "        print(f\"Evaluating: {example['instruction'][:100]}...\")\n",
    "        \n",
    "        prompt = format_prompt(example[\"instruction\"])\n",
    "        base_response = generate_response(base_model, base_tokenizer, prompt)\n",
    "        dpo_response = generate_response(dpo_eval_model, dpo_eval_tokenizer, prompt)\n",
    "        \n",
    "        #if cnt%10==0: \n",
    "        print(\"\\nDPO Model Response:\")\n",
    "        print(dpo_response)\n",
    "        print(\"\\nBase Model Response:\")\n",
    "        print(base_response)\n",
    "        \n",
    "        \n",
    "        # Check for empty or non-meaningful responses\n",
    "        base_empty = not base_response.strip() or len(base_response.strip()) < 5 or base_response.strip().lower() in [\"n/a\", \"none\", \"no response\"]\n",
    "        dpo_empty = not dpo_response.strip() or len(dpo_response.strip()) < 5 or dpo_response.strip().lower() in [\"n/a\", \"none\", \"no response\"]\n",
    "        \n",
    "        if base_empty and not dpo_empty:\n",
    "            verdict = 1\n",
    "            print(\"Base response empty or non-meaningful - verdict: 1 (DPO wins)\")\n",
    "        elif dpo_empty and not base_empty:\n",
    "            verdict = 2\n",
    "            print(\"DPO response empty or non-meaningful - verdict: 2 (Base wins)\")\n",
    "        elif base_empty and dpo_empty:\n",
    "            verdict = 0\n",
    "            print(\"Both responses empty or non-meaningful - verdict: 0 (tie)\")\n",
    "        elif base_response.strip() == dpo_response.strip():\n",
    "            verdict = 0\n",
    "            print(\"Responses identical - verdict: 0 (tie)\")\n",
    "        else:\n",
    "            verdict = get_judgment(example[\"instruction\"], dpo_response, base_response)\n",
    "            print(f\"\\nVerdict: {verdict} ({['Tie', 'DPO wins', 'Base wins'][verdict]})\")\n",
    "        \n",
    "        results.append({\n",
    "            \"instruction\": example[\"instruction\"],\n",
    "            \"base_response\": base_response,\n",
    "            \"dpo_response\": dpo_response,\n",
    "            \"verdict\": verdict\n",
    "        })\n",
    "        cnt += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "total = len(results)\n",
    "if total > 0:\n",
    "    base_wins = sum(1 for r in results if r[\"verdict\"] == 2)\n",
    "    dpo_wins = sum(1 for r in results if r[\"verdict\"] == 1)\n",
    "    ties = sum(1 for r in results if r[\"verdict\"] == 0)\n",
    "'''   \n",
    "    print(\"\\n=== Evaluationquet Summary ===\")\n",
    "    print(f\"Total examples evaluated: {total}\")\n",
    "    print(f\"Base Model wins: {base_wins} ({base_wins/total:.1%})\")\n",
    "    print(f\"DPO Model wins: {dpo_wins} ({dpo_wins/total:.1%})\")\n",
    "    print(f\"Ties: {ties} ({ties/total:.1%})\")\n",
    "'''\n",
    "\n",
    "# Calculate statistics\n",
    "verdicts = [r[\"verdict\"] for r in results]\n",
    "dpo_stats = {\n",
    "    \"base_wins\": sum(1 for v in verdicts if v == 2),\n",
    "    \"dpo_wins\": sum(1 for v in verdicts if v == 1),\n",
    "    \"ties\": sum(1 for v in verdicts if v == 0),\n",
    "    \"total\": len(verdicts),\n",
    "    \"dpo_win_rate\": sum(1 for v in verdicts if v == 1) / len(verdicts) if verdicts else 0,\n",
    "}\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== DPO Evaluation Summary ===\")\n",
    "print(f\"Total examples evaluated: {dpo_stats['total']}\")\n",
    "print(f\"Base Model wins: {dpo_stats['base_wins']} ({dpo_stats['base_wins']/dpo_stats['total']:.1%})\")\n",
    "print(f\"DPO Model wins: {dpo_stats['dpo_wins']} ({dpo_stats['dpo_wins']/dpo_stats['total']:.1%})\")\n",
    "print(f\"Ties: {dpo_stats['ties']} ({dpo_stats['ties']/dpo_stats['total']:.1%})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation completed in {time.time() - eval_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DPO Results\n",
    "Compare the DPO model performance against the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DPO results\n",
    "dpo_output_file = \"./outputs/alpacaeval_dpo_results.json\"\n",
    "\n",
    "with open(dpo_output_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"statistics\": dpo_stats,\n",
    "        \"results\": results,\n",
    "        \"config\": {\n",
    "            \"base_model\": \"unsloth/Llama-3.2-1B-unsloth-bnb-4bit\",\n",
    "            \"dpo_adapter\": \"/output/dpo_lora_adapter\",\n",
    "            \"judge_model\": \"unsloth/Llama-3.2-8B-Instruct-unsloth-bnb-4bit\",\n",
    "            \"num_samples\": len(results)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ [CHECKPOINT] DPO results saved to {dpo_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SFT and DPO Results\n",
    "Let's compare the performance of SFT and DPO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Model Comparison Summary ===\")\n",
    "print(f\"SFT Win Rate: {sft_stats['sft_win_rate']:.1%}\")\n",
    "print(f\"DPO Win Rate: {dpo_stats['dpo_win_rate']:.1%}\")\n",
    "print(f\"\\nSFT Performance: Base Wins: {sft_stats['base_wins']} | SFT Wins: {sft_stats['sft_wins']} | Ties: {sft_stats['ties']}\")\n",
    "print(f\"DPO Performance: Base Wins: {dpo_stats['base_wins']} | DPO Wins: {dpo_stats['dpo_wins']} | Ties: {dpo_stats['ties']}\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = (dpo_stats['dpo_win_rate'] - sft_stats['sft_win_rate']) * 100\n",
    "print(f\"\\nDPO improvement over SFT: {improvement:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "Print a final summary of the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"SFT Win Rate: {sft_stats['sft_win_rate']:.1%}\")\n",
    "print(f\"DPO Win Rate: {dpo_stats['dpo_win_rate']:.1%}\")\n",
    "print(f\"DPO Improvement over SFT: {improvement:.1f} percentage points\")\n",
    "print(f\"\\nFULL PIPELINE COMPLETED SUCCESSFULLY in {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORTANT**: \n",
    "Remember to upload your model to HuggingFace!. \n",
    "**If you're using LORA, make sure to merge your model with your Lora adapters before pushing to HuggingFace. Otherwise, evaluation will automatically be carried out on just the base model.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
